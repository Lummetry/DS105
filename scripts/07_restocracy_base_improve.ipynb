{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a804f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2377b7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.21.5\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57ffbf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85c9e1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd87836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = '..\\data'\n",
    "FN = 'restocracy_all_data.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "562ad53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(FOLDER, FN), 'rb') as fh:\n",
    "    data = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ea71171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16e1bf5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4df5aff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'price': '202 lei',\n",
       " 'review': ' \\n Noul restaurant vedeta al Bucurestiului, o constructie geniala de marketing \\n Maize s-a deschis la inceputul lui noiembrie 2017 in mansarda unui (fel de) bloc cu doua etaje de pe eleganta strada Paris, aproape de Piata Dorobantilor. Poti sa o iei, la fel de bine, si ca pe o casa veche din Bucurestiul vechi, facuta fara imaginatie, sau cu economie la arhitect. Casa e renovata recent, iar zona e atat de frumoasa si de aristocratica, incat orice casa de acolo nu poate sa fie decat frumoasa si ea, indiferent cum ar arata si in ce stare ar fi. \\n Daca cei care au construit batrana casa nu au adus arhitecti cu imaginatie, cei care au transformat mansarda in restaurantul Maize au facut-o. Totul arata foarte bine acolo, au pus in valoare fiecare coltisor si detaliu, au reusit sa o umple si de lumina, si au facut loc si bucatariei chiar in mijloc, cu plite si gratare cu tot, dupa cum se vede si in poze, care ma scutesc de mai mult scris. Acolo, sus, au si o mica terasa pe acoperis, foarte frumoasa si ea, desigur, si nu stiu daca a lor va fi si curtea din spate: mare, cu case frumoase in jur, un alt arhitect bun ar putea face acolo o terasa de referinta a orasului. Muzica e data foarte tare, dar presupun ca am fost printre putinii deranjati de asta. \\n Dar sa vorbim despre mancare, pentru ca la Maize, spre diferenta de multe alte restaurante care au investit o avere in amenajare, mancarea e un lucru serios, chiar foarte, foarte serios. Au la Maize un chef, nu stiu cum il cheama, dar cred ca e cel despre care am mai scris pe cand a deschis un restaurant in apropiere de Marriott, ceva cu Funky Cuisine, si apoi l-am vazut, pentru scurt timp, la E3 by Entourage, in Piata Floreasca. Daca e acelasi, deci, atunci e foarte talentat, am scris si atunci asta, si acum nu fac decat sa confirm ca nu a stat pe loc, ci a mers mult mai departe. Si nu se va opri aici, fara indoiala… E unul dintre cei mai creativi si mai potriviti pentru o astfel de meserie din cati bucatari am vazut. \\n Conceptul gastronomic enuntat de Maize, si sustinut de tot ce am vazut in meniu, pe plita si in farfurie, este de „farm to table”, de „bucatarie creativa de ingrediente” romanesti, adica inventarea, recrearea, recompunerea si recombinarea retetelor si crearea altora noi, cam tot ce se poate face cu niste produse si ingrediente de buna calitate venite de la furnizori seriosi si de incredere – atat cat se poate. Putem sa-i spunem, la fel de bine, La Nouvelle Cuisine Roumaine/ Romaneasca, bucatari fina, adica, si atunci ei ar fi pionierii, deschizatorii de drum, fara indoiala. \\n Meniul e cu totul original, nu cred ca e un singur fel acolo cu ceva ce s-a mai facut in alte restaurante. Si in asta sta (o parte din) talentul chefului despre care am vorbit, sa recompuna la nesfarsit produsele si ingredientele si sa puna in farfurie combinatiile cele mai reusite. \\n Ca totul e la vedere la Maize, ca sase bucatari – toti tineri si frumosi, cu fete si tatuaje de pus in revista, nu in ultimul rand – muncesc la fiecare farfurie in parte de parca ar fi intr-un atelier de orfevarie, e iarasi o constructie a unui mare specialist in marketing – daca geniul in marketing nu o fi insusi cheful… \\n Tot ce am mancat la Maize a fost bun si foarte bun. Ne-au dat ca amuse-bouche o zacusca, pusa pe un fel de turta mica. Foarte buna la gust, insa cu zahar in exces, parca mancai Nutella pe paine. Orezul cu parmezan 9si cu galbenus de ou, presupun) foarte bun, de asemenea, tartarul de vita original, bun si el; nu cred ca avea untura sau seu de vita in el, insa asa parea. Un file de dorada cu sos a fost revelatia zilei, foarte buna dorada, excelent sosul, foarte bine facut spanacul. Apoi am primit o ceafa de porc de Mangalita, putin cam prea mult facut, cu o varza tinuta prea mult pe o plita prea incinsa, dupa cum se vede si din poza, cu cateva frunze de ceva prea sarate, iar cu restul legumelor foarte bune si bune facute. Si desrtul a fost foarte bun, si chiar si cafeaua, o surpriza placuta si un lucru prea mult neglijat de catre cei din resturante. \\n Interesant a fost si ca unul dintre bucatari – si nu chelnerul, si aici iarasi se vede intuitia geniala de marketing si cunoasterea detaliilor fine ale client service-ului – venea la masa si explica fiecare fel in parte. \\n Serviciul a fost bun, cu un chelner relaxat si placut la apropiere, dar care nu aflase inca, la o varsta nu tocmai de incepator, ca nu mananca doi oameni cu furculitele direct dintr-o aceeasi farfurie, chiar daca sunt ei prieteni buni, de-o viata. Dar nu s-a prins el nici dupa ce i-am explicat noi cum e cu igiena in farfurie… \\n Preturile sunt normale si rezonabile, daca e sa masuram ce vezi in farfurie si in jur la Maize fata de alte restaurante de pe strazile scumpe ale Bucurestiului. O masa in doi, cu o sticla de vin bun, te costa vreo 4-500 de lei. \\n Maize, baietii lor frumosi si foarte talentati din bucatarie, si, mai ales, farfuriile cu mancaruri ca acelea din pozele noastre, vor deveni in foarte scurt timp vedete de prim-plan ale gastronomiei si mondenitatii bucurestene, fara indoiala. ii voi urmari atent si voi mai scrie despre ei, cu atat mai mult cu cat sunt foarte aproape de mine… (GB – noiembrie 2017) \\n \\n\\t\\t\\t\\tVineri, 1 noiembrie 2018, Asociatia Producatorilor de Salam de Sibiu (APSS) a organizat o cina festiva pentru a sarbatori cei 130 de ani de excelenta si recunoastere europeana de care se bucura acest ...\\t\\t\\t \\n \\n\\t\\t\\t\\tDiferenta dintre marile restaurante si celelalte o vezi, cu adevarat, la legume si la sosuri. Nu la carne, la peste si la ce mai poti primi intr-o farfurie. Replica lui Paul Bocuse, cum ca \"un peste e...\\t\\t\\t \\n \\n\\t\\t\\t\\t Cand am dat Premiile Restocracy pe 2017 pentru restaurantul cu cea mai buna bucatarie romaneasca din Bucuresti, nu am luat in calcul diferentele dintre restaurantele traditionale romanesti, cele cu b...\\t\\t\\t \\n \\n\\t\\t\\t\\tMulte bucatarii nationale au cele doua ramuri bine delimitate: o bucatarie traditionala, populara, si o bucatarie fina. Incepand cu cea mai celebra, mai sofisticata si mai buna din lume, La Cuisine Fr...\\t\\t\\t \\n \\n\\t\\t\\t\\tAlex Petricean este Head Chef-ul restaurantului Maize, deschis anul acesta.\\r\\n\\r\\nRestocracy: De unde, sau de la cine, ati invatat cele mai importante lucruri pe care le stiti despre bucatarie si gatit?\\r...\\t\\t\\t \\n MAIZE si-a deschis, in sfarsit, terasa, dupa foarte multe dificultati tehnice pe care au trebuit sa le depaseasca, fiindca au tinut sa faca si bucatarie acolo, chiar daca una restransa. E in curtea din spate a cladirii din strada Paris 61 in care sunt. Au facut o bucatarie de vara, o bucatarie deschisa. Ce se serveste pe terasa se pregateste direct acolo, un meniu mai restrans decat cel din restaurant. Au vreo zece mese mici, din cate am numarat eu, cu banchete la perete, incap vreo 25 de oameni in toata terasa, cel mult. Pozele sunt de la evenimentul de inaugurare, din 2 septembrie. (GB – sept. 2018)'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed1a8efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = [x['price'] for x in data]\n",
    "reviews = [x['review'] for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87120007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['202 lei', '176 lei', '187 lei', '264 lei', '187 lei']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2837737a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n Noul restaurant vedeta al Bucurestiului, o constructie geniala de marketing \\n Maize s-a deschis l'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c702897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([202, 176, 187, 264, 187, 160, 133, 182, 256, 288, 266, 229, 133,\n",
       "       214, 109, 219, 147, 200, 170, 360, 258, 232, 133, 168, 206, 232,\n",
       "       133, 168, 206, 138, 138, 106, 181, 208, 120, 133, 186, 126, 149,\n",
       "        82, 130, 189, 114, 208, 184, 226, 118, 130, 155, 141, 128, 141,\n",
       "       387, 104, 106, 154, 155, 246, 154, 128, 102, 109, 114, 162, 194,\n",
       "       309, 176, 162, 224, 126, 104,  64, 114, 117, 130, 130, 149,  82,\n",
       "       107, 229, 253,  91, 187,  91, 251, 131, 142, 114, 138, 115, 109,\n",
       "       130, 115, 259, 115,  90, 136, 122, 150, 144, 131, 138, 158, 114,\n",
       "       117, 114, 110, 131, 162, 106, 218, 101, 109, 144, 134, 149, 138,\n",
       "       130, 101, 115, 133,  96, 165, 120,  70,  88,  98, 101, 149,  94,\n",
       "       226, 163, 115,  94,  91,  82, 117,  99, 122, 163,  67, 123, 149,\n",
       "       134, 142,  96, 224,  96, 168, 136, 146, 128,  94, 126, 170, 139,\n",
       "       109, 123, 107, 115, 128, 104, 146, 197, 122, 150, 128, 106, 130,\n",
       "       131, 131,  85, 122, 117,  83,  86, 123, 109, 112, 112, 130, 109,\n",
       "       162, 126, 136, 120,  77, 107,  80, 107, 112, 117, 109, 122,  83,\n",
       "       115, 173,  85, 115, 189])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_prices = np.array([int(x.split(' ')[0]) for x in prices])\n",
    "np_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74c6ab77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_prices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9548abb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3., 11., 16., 37., 26., 32., 16., 11.,  8.,  9.,  6.,  5.,  6.,\n",
       "         2.,  4.,  4.,  0.,  1.,  1.,  0.,  0.,  0.,  1.,  0.,  1.]),\n",
       " array([ 64.  ,  76.92,  89.84, 102.76, 115.68, 128.6 , 141.52, 154.44,\n",
       "        167.36, 180.28, 193.2 , 206.12, 219.04, 231.96, 244.88, 257.8 ,\n",
       "        270.72, 283.64, 296.56, 309.48, 322.4 , 335.32, 348.24, 361.16,\n",
       "        374.08, 387.  ]),\n",
       " <BarContainer object of 25 artists>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: plot prices histogram, 25 bins\n",
    "plt.hist(np_prices, bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9c0b6bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Noul', 'restaurant', 'vedeta', 'al', 'Bucurestiului,']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[0].split()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40289f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(all_docs):\n",
    "    dct_occurences = {}\n",
    "    for doc in all_docs:\n",
    "        words_in_doc = doc.split()\n",
    "        for word in words_in_doc:\n",
    "            if word in dct_occurences:\n",
    "                dct_occurences[word] += 1\n",
    "            else:\n",
    "                dct_occurences[word] = 1\n",
    "    lst_occ = sorted([(w, occ) for w, occ in dct_occurences.items()], key=lambda x: x[1], reverse=True)\n",
    "    dct_word_to_index = {w[0]:idx+1 for idx, w in enumerate(lst_occ)}\n",
    "    dct_word_to_index['UNK'] = 0\n",
    "    dct_index_to_word = {i:w for w,i in dct_word_to_index.items()}\n",
    "    return lst_occ, dct_word_to_index, dct_index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d7a3879",
   "metadata": {},
   "outputs": [],
   "source": [
    "o, w2i, i2w = get_vocabulary(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef0e9f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('de', 7642),\n",
       " ('si', 6195),\n",
       " ('in', 3904),\n",
       " ('la', 3650),\n",
       " ('cu', 3275),\n",
       " ('mai', 3019),\n",
       " ('ca', 2698),\n",
       " ('sa', 2655),\n",
       " ('din', 2230),\n",
       " ('care', 2147)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cae756cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2w[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9aead04f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UNK'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2w[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5380157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2i['restaurant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0f9c427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4., 19., 41., 50., 35.,  7.,  7.,  8.,  5.,  2.,  3.,  1.,  3.,\n",
       "         3.,  2.,  1.,  0.,  0.,  0.,  1.,  2.,  0.,  2.,  0.,  2.,  0.,\n",
       "         0.,  0.,  0.,  2.]),\n",
       " array([  34.,  205.,  376.,  547.,  718.,  889., 1060., 1231., 1402.,\n",
       "        1573., 1744., 1915., 2086., 2257., 2428., 2599., 2770., 2941.,\n",
       "        3112., 3283., 3454., 3625., 3796., 3967., 4138., 4309., 4480.,\n",
       "        4651., 4822., 4993., 5164.]),\n",
       " <BarContainer object of 30 artists>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reviews_lens = [len(x.split()) for x in reviews]\n",
    "plt.hist(reviews_lens, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "222efe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_tokens(review, dct_w2i, unk_word_idx=0, max_words=1000):\n",
    "    obs_tokens = []\n",
    "    for word in review.split():\n",
    "        obs_tokens.append(dct_w2i.get(word, unk_word_idx))\n",
    "    diff = max(0, max_words - len(obs_tokens))\n",
    "    obs_tokens = obs_tokens + [unk_word_idx] * diff\n",
    "    obs_tokens = obs_tokens[:max_words]\n",
    "    return obs_tokens\n",
    "\n",
    "def tokens_to_review(tokens, dct_i2w):\n",
    "    # TODO: complete list comprehension for generating review from tokens\n",
    "    return \" \".join([dct_i2w[t] for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7374009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2181, 26, 2751, 37, 244, 15, 1377, 4219, 1, 2420]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o1 = review_to_tokens(reviews[0], dct_w2i=w2i)\n",
    "o1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a43ef076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Noul restaurant vedeta al Bucurestiului, o constructie geniala de marketing Maize s-a deschis la inceputul lui noiembrie 2017 in mansarda unui (fel de) bloc cu doua etaje de pe eleganta strada Paris, aproape de Piata Dorobantilor. Poti sa o iei, la fel de bine, si ca pe o casa veche din Bucurestiul vechi, facuta fara imaginatie, sau cu economie la arhitect. Casa e renovata recent, iar zona e atat de frumoasa si de aristocratica, incat orice casa de acolo nu poate sa fie decat frumoasa si ea, indiferent cum ar arata si in ce stare ar fi. Daca cei care au construit batrana casa nu au adus arhitecti cu imaginatie, cei care au transformat mansarda in restaurantul Maize au facut-o. Totul arata foarte bine acolo, au pus in valoare fiecare coltisor si detaliu, au reusit sa o umple si de lumina, si au facut loc si bucatariei chiar in mijloc, cu plite si gratare cu tot, dupa cum se vede si in poze, care ma scutesc de mai mult scris. Acolo, sus, au si o mica terasa pe acoperis, foarte frumoasa si ea, desigur, si nu stiu daca a lor va fi si curtea din spate: mare, cu case frumoase in jur, un alt arhitect bun ar putea face acolo o terasa de referinta a orasului. Muzica e data foarte tare, dar presupun ca am fost printre putinii deranjati de asta. Dar sa vorbim despre mancare, pentru ca la Maize, spre diferenta de multe alte restaurante care au investit o avere in amenajare, mancarea e un lucru serios, chiar foarte, foarte serios. Au la Maize un chef, nu stiu cum il cheama, dar cred ca e cel despre care am mai scris pe cand a deschis un restaurant in apropiere de Marriott, ceva cu Funky Cuisine, si apoi l-am vazut, pentru scurt timp, la E3 by Entourage, in Piata Floreasca. Daca e acelasi, deci, atunci e foarte talentat, am scris si atunci asta, si acum nu fac decat sa confirm ca nu a stat pe loc, ci a mers mult mai departe. Si nu se va opri aici, fara indoiala… E unul dintre cei mai creativi si mai potriviti pentru o astfel de meserie din cati bucatari am vazut. Conceptul gastronomic enuntat de Maize, si sustinut de tot ce am vazut in meniu, pe plita si in farfurie, este de „farm to table”, de „bucatarie creativa de ingrediente” romanesti, adica inventarea, recrearea, recompunerea si recombinarea retetelor si crearea altora noi, cam tot ce se poate face cu niste produse si ingrediente de buna calitate venite de la furnizori seriosi si de incredere – atat cat se poate. Putem sa-i spunem, la fel de bine, La Nouvelle Cuisine Roumaine/ Romaneasca, bucatari fina, adica, si atunci ei ar fi pionierii, deschizatorii de drum, fara indoiala. Meniul e cu totul original, nu cred ca e un singur fel acolo cu ceva ce s-a mai facut in alte restaurante. Si in asta sta (o parte din) talentul chefului despre care am vorbit, sa recompuna la nesfarsit produsele si ingredientele si sa puna in farfurie combinatiile cele mai reusite. Ca totul e la vedere la Maize, ca sase bucatari – toti tineri si frumosi, cu fete si tatuaje de pus in revista, nu in ultimul rand – muncesc la fiecare farfurie in parte de parca ar fi intr-un atelier de orfevarie, e iarasi o constructie a unui mare specialist in marketing – daca geniul in marketing nu o fi insusi cheful… Tot ce am mancat la Maize a fost bun si foarte bun. Ne-au dat ca amuse-bouche o zacusca, pusa pe un fel de turta mica. Foarte buna la gust, insa cu zahar in exces, parca mancai Nutella pe paine. Orezul cu parmezan 9si cu galbenus de ou, presupun) foarte bun, de asemenea, tartarul de vita original, bun si el; nu cred ca avea untura sau seu de vita in el, insa asa parea. Un file de dorada cu sos a fost revelatia zilei, foarte buna dorada, excelent sosul, foarte bine facut spanacul. Apoi am primit o ceafa de porc de Mangalita, putin cam prea mult facut, cu o varza tinuta prea mult pe o plita prea incinsa, dupa cum se vede si din poza, cu cateva frunze de ceva prea sarate, iar cu restul legumelor foarte bune si bune facute. Si desrtul a fost foarte bun, si chiar si cafeaua, o surpriza placuta si un lucru prea mult neglijat de catre cei din resturante. Interesant a fost si ca unul dintre bucatari – si nu chelnerul, si aici iarasi se vede intuitia geniala de marketing si cunoasterea detaliilor fine ale client service-ului – venea la masa si explica fiecare fel in parte. Serviciul a fost bun, cu un chelner relaxat si placut la apropiere, dar care nu aflase inca, la o varsta nu tocmai de incepator, ca nu mananca doi oameni cu furculitele direct dintr-o aceeasi farfurie, chiar daca sunt ei prieteni buni, de-o viata. Dar nu s-a prins el nici dupa ce i-am explicat noi cum e cu igiena in farfurie… Preturile sunt normale si rezonabile, daca e sa masuram ce vezi in farfurie si in jur la Maize fata de alte restaurante de pe strazile scumpe ale Bucurestiului. O masa in doi, cu o sticla de vin bun, te costa vreo 4-500 de lei. Maize, baietii lor frumosi si foarte talentati din bucatarie, si, mai ales, farfuriile cu mancaruri ca acelea din pozele noastre, vor deveni in foarte scurt timp vedete de prim-plan ale gastronomiei si mondenitatii bucurestene, fara indoiala. ii voi urmari atent si voi mai scrie despre ei, cu atat mai mult cu cat sunt foarte aproape de mine… (GB – noiembrie 2017) Vineri, 1 noiembrie 2018, Asociatia Producatorilor de Salam de Sibiu (APSS) a organizat o cina festiva pentru a sarbatori cei 130 de ani de excelenta si recunoastere europeana de care se bucura acest ... Diferenta dintre marile restaurante si celelalte o vezi, cu adevarat, la legume si la sosuri. Nu la carne, la peste si la ce'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_review(o1, i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1dad1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([202, 176, 187, 264, 187, 160, 133, 182, 256, 288])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_prices[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db97e2b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1000])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: plot histogram of review lenghts\n",
    "lst_obs = [review_to_tokens(review=x, dct_w2i=w2i) for x in reviews]\n",
    "lens = [len(x) for x in lst_obs]\n",
    "np.unique(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6acc28dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[57, 24, 4, 12, 0, 4, 10, 18, 257, 891, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_review = 'Am fost la un restarant la care am mancat sushi'\n",
    "new_review_tokens = review_to_tokens(new_review, w2i)\n",
    "new_review_tokens[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ee5822b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Am fost la un UNK la care am mancat sushi UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_review(new_review_tokens, i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4172a7f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = [review_to_tokens(x, w2i) for x in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9756e5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 1000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_X = np.array(X)\n",
    "np_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c9ea31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10725,     5,  3085,     4,   277,     2,     4,  6841,    54,     4,   887,     4,    65,     2,     4,    19],\n",
       "       [    9,    68,    49,  6883,  1998,  1545,  1001,   171,   308,     9,   478,  1388,  1801,   533,     5,  4254],\n",
       "       [ 1808,   171, 10859,  3622,    25,   902,     1,  1462,   116, 10860,     1,     4,  5265,     4, 10861,     2],\n",
       "       [  152,     3,  1564,  6975,    30,   702,   245,  2207,    25,     1,   128,   334,    85,     3,   457,   122],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [ 5390,    17,   167,   368,   304,     8,    79,  7170,   152,    24,  1380,    85,   389,   116,   349,   375],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [    1,  1326,    47,     2,     5,  1126,     6,  5471,     1,  2025,  1128,     3,   331,    13,    23,    12],\n",
       "       [  431,  5379,    27,    47,    12,   431,    69,  7153,  7154,    20,   114,  5380,   243,  3176,     2,  3177],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [11860,  7602,   214,  1206,   202,  3116, 11861,   335,   906,  3817, 11862,  3812,  1599, 11863,  1884,   685],\n",
       "       [  204, 12265,    14,  3809,  5720,   202,  7719,  3290, 12266,   202, 12267,  3826,  1599,  5653,   262, 12268],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [  109,     8,   118, 12481, 12482,     1,   147,  1291,   791,   164,   274,   701,     5,    10,    18,   479]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(precision=5, suppress=True, linewidth=500)\n",
    "np_X[:20,-16:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f6142a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([202, 176, 187, 264, 187, 160, 133, 182, 256, 288, 266, 229, 133, 214, 109, 219, 147, 200, 170, 360, 258, 232, 133, 168, 206, 232, 133, 168, 206, 138, 138, 106, 181, 208, 120, 133, 186, 126, 149,  82, 130, 189, 114, 208, 184, 226, 118, 130, 155, 141, 128, 141, 387, 104, 106, 154, 155, 246, 154, 128, 102, 109, 114, 162, 194, 309, 176, 162, 224, 126, 104,  64, 114, 117, 130, 130, 149,  82, 107, 229, 253,  91, 187,  91, 251, 131, 142, 114, 138, 115, 109, 130, 115, 259, 115,  90, 136, 122,\n",
       "       150, 144, 131, 138, 158, 114, 117, 114, 110, 131, 162, 106, 218, 101, 109, 144, 134, 149, 138, 130, 101, 115, 133,  96, 165, 120,  70,  88,  98, 101, 149,  94, 226, 163, 115,  94,  91,  82, 117,  99, 122, 163,  67, 123, 149, 134, 142,  96, 224,  96, 168, 136, 146, 128,  94, 126, 170, 139, 109, 123, 107, 115, 128, 104, 146, 197, 122, 150, 128, 106, 130, 131, 131,  85, 122, 117,  83,  86, 123, 109, 112, 112, 130, 109, 162, 126, 136, 120,  77, 107,  80, 107, 112, 117, 109, 122,  83, 115,\n",
       "       173,  85, 115, 189])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_y = np_prices\n",
    "np_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e8f5ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: prepare train/test\n",
    "from sklearn.model_selection import train_test_split as splitter\n",
    "x_trn, x_tst, y_trn, y_tst = splitter(np_X, np_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "294c3024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 1000)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_trn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6fad1646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 1000)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "652956d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1000)\n",
      "(20, 1000)\n"
     ]
    }
   ],
   "source": [
    "x_dev = x_tst[:20]\n",
    "y_dev = y_tst[:20]\n",
    "x_test = x_tst[20:]\n",
    "y_test = y_tst[20:]\n",
    "print(x_dev.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28d93d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3517,    12,    26,    17,   610],\n",
       "       [ 6802,    15,  1205,  2137,  8998],\n",
       "       [16910, 16911, 16912,    23, 16913]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_trn[:3,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84f00129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For vocab size 22000, naive (one-hote) data size is 4 GB\n",
      "For embeds: 7 MB\n"
     ]
    }
   ],
   "source": [
    "EMBED_SIZE = 10\n",
    "print(\"For vocab size {}, naive (one-hote) data size is {} GB\".format(len(w2i), len(reviews) * x_trn.shape[1] * len(w2i) // (1024**3)))\n",
    "print(\"For embeds: {} MB\".format(len(reviews) * x_trn.shape[1] * EMBED_SIZE * 4 // (1024**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "47551b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"token_inputs_3:0\", shape=(None, 1000), dtype=int32)\n",
      "Tensor(\"tokens_to_vectors/embedding_lookup_3/Identity_1:0\", shape=(None, 1000, 10), dtype=float32)\n",
      "Tensor(\"matrix_to_vector/Reshape_2:0\", shape=(None, 10000), dtype=float32)\n",
      "Tensor(\"linear_transform/BiasAdd:0\", shape=(None, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf_inp = tf.keras.layers.Input((1_000,), name='token_inputs', dtype=tf.int32,)\n",
    "print(tf_inp)\n",
    "# TODO: project from vocab size to continous semantic space\n",
    "layer_emb = tf.keras.layers.Embedding(len(w2i), EMBED_SIZE,  name='tokens_to_vectors')\n",
    "tf_emb = layer_emb(tf_inp)\n",
    "print(tf_emb)\n",
    "# TODO: flatten to one vector per obs\n",
    "tf_x = tf.keras.layers.Flatten(name='matrix_to_vector')(tf_emb)\n",
    "print(tf_x)\n",
    "# TODO: linear transform\n",
    "tf_x = tf.keras.layers.Dense(64, name='linear_transform')(tf_x)\n",
    "print(tf_x)\n",
    "# TODO: activate\n",
    "tf_x = tf.keras.layers.Activation('relu', name='relu_activation')(tf_x)\n",
    "\n",
    "tf_x = tf.keras.layers.Dropout(0.5, name='pre_readout_dropout')(tf_x)\n",
    "# finally we have our regression output\n",
    "tf_out = tf.keras.layers.Dense(1, name='readout')(tf_x)\n",
    "model = tf.keras.models.Model(tf_inp, tf_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "53baf42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "token_inputs (InputLayer)    [(None, 1000)]            0         \n",
      "_________________________________________________________________\n",
      "tokens_to_vectors (Embedding (None, 1000, 10)          220000    \n",
      "_________________________________________________________________\n",
      "matrix_to_vector (Flatten)   (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "linear_transform (Dense)     (None, 64)                640064    \n",
      "_________________________________________________________________\n",
      "relu_activation (Activation) (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "pre_readout_dropout (Dropout (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "readout (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 860,129\n",
      "Trainable params: 860,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a7a3dab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b37a6d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 140.4699 - val_loss: 140.2502\n",
      "Epoch 2/50\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 129.0073 - val_loss: 118.8731\n",
      "Epoch 3/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 99.5901 - val_loss: 71.2637\n",
      "Epoch 4/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 57.2300 - val_loss: 34.2118\n",
      "Epoch 5/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 49.3695 - val_loss: 32.3123\n",
      "Epoch 6/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 47.0985 - val_loss: 33.1912\n",
      "Epoch 7/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 39.2726 - val_loss: 33.9964\n",
      "Epoch 8/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 35.8492 - val_loss: 31.0868\n",
      "Epoch 9/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 34.2754 - val_loss: 36.4048\n",
      "Epoch 10/50\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 37.2742 - val_loss: 34.5826\n",
      "Epoch 11/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 32.6463 - val_loss: 35.3939\n",
      "Epoch 12/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 35.6268 - val_loss: 34.4615\n",
      "Epoch 13/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 30.2097 - val_loss: 32.9286\n",
      "Epoch 14/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 28.2207 - val_loss: 33.8616\n",
      "Epoch 15/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 32.0405 - val_loss: 30.7539\n",
      "Epoch 16/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 28.5335 - val_loss: 33.8562\n",
      "Epoch 17/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 31.6888 - val_loss: 35.3055\n",
      "Epoch 18/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 27.4241 - val_loss: 34.3558\n",
      "Epoch 19/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 30.0046 - val_loss: 37.7803\n",
      "Epoch 20/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 26.9715 - val_loss: 31.5272\n",
      "Epoch 21/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 25.9268 - val_loss: 33.0052\n",
      "Epoch 22/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 22.8183 - val_loss: 32.5781\n",
      "Epoch 23/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 25.0173 - val_loss: 32.6582\n",
      "Epoch 24/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 23.6031 - val_loss: 34.7353\n",
      "Epoch 25/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 24.6188 - val_loss: 30.9596\n",
      "Epoch 26/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 23.5210 - val_loss: 33.8824\n",
      "Epoch 27/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 26.9852 - val_loss: 30.7782\n",
      "Epoch 28/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 22.8221 - val_loss: 34.2318\n",
      "Epoch 29/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 23.4473 - val_loss: 29.2257\n",
      "Epoch 30/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 25.5439 - val_loss: 32.2075\n",
      "Epoch 31/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 21.4914 - val_loss: 30.6966\n",
      "Epoch 32/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 21.7818 - val_loss: 29.5673\n",
      "Epoch 33/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 22.1174 - val_loss: 33.4687\n",
      "Epoch 34/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 22.2356 - val_loss: 29.0662\n",
      "Epoch 35/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 23.2958 - val_loss: 30.1680\n",
      "Epoch 36/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 23.2701 - val_loss: 31.0662\n",
      "Epoch 37/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 21.6006 - val_loss: 29.2628\n",
      "Epoch 38/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 20.9073 - val_loss: 30.8442\n",
      "Epoch 39/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 21.4368 - val_loss: 29.1292\n",
      "Epoch 40/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 25.1039 - val_loss: 28.2596\n",
      "Epoch 41/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 20.2716 - val_loss: 32.6907\n",
      "Epoch 42/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 22.3996 - val_loss: 29.5110\n",
      "Epoch 43/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 21.3131 - val_loss: 29.8343\n",
      "Epoch 44/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 22.2727 - val_loss: 29.4562\n",
      "Epoch 45/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 19.6498 - val_loss: 30.4137\n",
      "Epoch 46/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 21.4625 - val_loss: 31.0292\n",
      "Epoch 47/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 21.2067 - val_loss: 28.4598\n",
      "Epoch 48/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 19.4293 - val_loss: 30.9944\n",
      "Epoch 49/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 21.5096 - val_loss: 28.8156\n",
      "Epoch 50/50\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 21.3352 - val_loss: 31.7737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x260d5fc7af0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_trn, y=y_trn, validation_data=(x_dev, y_dev), epochs=50, batch_size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "56a0c38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 83 186 117 117 181 114 130 123 126 168  96 206  91 387 149 288  91 109 194 126]\n",
      "[116 150 139 123 135 127 122 137 117 161 124 188 114 156 134 118 102 120 108 150]\n",
      "[0.40363 0.19025 0.19363 0.0575  0.25166 0.12062 0.05808 0.11995 0.06464 0.03964 0.29414 0.08502 0.25779 0.59638 0.09682 0.58775 0.12435 0.1036  0.43883 0.19049]\n"
     ]
    }
   ],
   "source": [
    "yh_test = model.predict(x_test)\n",
    "print(y_test.ravel())\n",
    "print(yh_test.ravel().astype('int'))\n",
    "mean_prc_err = np.abs(yh_test.ravel() - y_test.ravel()) / y_test.ravel()\n",
    "print(mean_prc_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36030e66",
   "metadata": {},
   "source": [
    "### TODO: \n",
    " - prepare grid search\n",
    " - write functions for result qualitative analysis\n",
    " - explore solution space and show performance results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3faec575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.keras.layers.pooling.GlobalMaxPooling1D"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.layers.GlobalMaxPool1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "37599935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_factory(inp, \n",
    "                  vocab,  \n",
    "                  layers=[32,8], conv=False, embed_size=10, activation='relu', \n",
    "                  dropout_readout=0.5, dropout_hidden=0, multi_column_conv=False,\n",
    "                  optim='adam', loss='mse'\n",
    "                 ):\n",
    "    tf_inp = tf.keras.layers.Input((inp,), name='token_inputs', dtype=tf.int32,)\n",
    "    layer_emb = tf.keras.layers.Embedding(vocab, embed_size,  name='tokens_to_vectors')\n",
    "    tf_x = layer_emb(tf_inp)\n",
    "    if not conv:\n",
    "        tf_x = tf.keras.layers.Flatten(name='matrix_to_vector')(tf_x)        \n",
    "    else:\n",
    "        layers = reversed(layers)\n",
    "        # three conv columns\n",
    "        tf_x1, tf_x2, tf_x3 = tf_x, tf_x, tf_x\n",
    "    for i, layer in enumerate(layers):\n",
    "        if conv:\n",
    "            # now we create a few of convolution on same data\n",
    "            tf_x1 = tf.keras.layers.Conv1D(filters=layer, kernel_size=3, strides=3, name='c1conv_{}_f{}'.format(i, layer))(tf_x1)\n",
    "            if multi_column_conv:\n",
    "                tf_x2 = tf.keras.layers.Conv1D(filters=layer, kernel_size=5, strides=5, name='c2conv_{}_f{}'.format(i, layer))(tf_x2)\n",
    "                tf_x3 = tf.keras.layers.Conv1D(filters=layer, kernel_size=7, strides=7, name='c3conv_{}_f{}'.format(i, layer))(tf_x3)\n",
    "        else:\n",
    "            tf_x = tf.keras.layers.Dense(layer, name='linear_{}_{}'.format(i, layer))(tf_x)\n",
    "            tf_x = tf.keras.layers.Activation(activation, name='{}_{}'.format(activation, i))(tf_x)\n",
    "            if dropout_hidden > 0 and i < (len(layers) - 1):\n",
    "                tf_x = tf.keras.layers.Dropout(0.5, name='hid_drop_{}_{}'.format(i, dropout_hidden))(tf_x)\n",
    "    if conv:\n",
    "        tf_x1 = tf.keras.layers.GlobalMaxPool1D(name='gmp1')(tf_x1)\n",
    "        if multi_column_conv:\n",
    "            tf_x2 = tf.keras.layers.GlobalMaxPool1D(name='gmp2')(tf_x2)\n",
    "            tf_x3 = tf.keras.layers.GlobalMaxPool1D(name='gmp3')(tf_x3)\n",
    "            tf_x = tf.keras.layers.concatenate([tf_x1, tf_x2, tf_x3])\n",
    "        else:\n",
    "            tf_x = tf_x1\n",
    "    if dropout_readout > 0:\n",
    "        tf_x = tf.keras.layers.Dropout(0.5, name='pre_readout_drop_{}'.format(dropout_readout))(tf_x)\n",
    "    \n",
    "    tf_out = tf.keras.layers.Dense(1, name='readout')(tf_x)\n",
    "    m = tf.keras.models.Model(tf_inp, tf_out)\n",
    "    m.compile(optimizer=optim, loss=loss)\n",
    "    m.summary()\n",
    "    return m\n",
    "\n",
    "def final_eval(m, x_test, y_test):\n",
    "    yh_test = m.predict(x_test)\n",
    "    y_test = y_test.ravel()\n",
    "    yh_test = yh_test.ravel()\n",
    "    return (np.abs(yh_test - y_test) / y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0d54e105",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_grid_space = {\n",
    "    'layers' : [\n",
    "        [128, 32, 8],\n",
    "        [64],\n",
    "        [64, 8],\n",
    "    ],\n",
    "    'conv' : [True, False],\n",
    "    'embed_size' : [8, 32],\n",
    "}\n",
    "\n",
    "grid_params = []\n",
    "grid_values = []\n",
    "for k in dct_grid_space:\n",
    "    grid_params.append(k)\n",
    "    grid_values.append(dct_grid_space[k])\n",
    "import itertools\n",
    "grid_combs = list(itertools.product(*grid_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6f52caba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running iteration 1/12: {'layers': [128, 32, 8], 'conv': True, 'embed_size': 8}\n",
      "Model: \"functional_243\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "token_inputs (InputLayer)    [(None, 1000)]            0         \n",
      "_________________________________________________________________\n",
      "tokens_to_vectors (Embedding (None, 1000, 8)           176000    \n",
      "_________________________________________________________________\n",
      "c1conv_0_f8 (Conv1D)         (None, 333, 8)            200       \n",
      "_________________________________________________________________\n",
      "c1conv_1_f32 (Conv1D)        (None, 111, 32)           800       \n",
      "_________________________________________________________________\n",
      "c1conv_2_f128 (Conv1D)       (None, 37, 128)           12416     \n",
      "_________________________________________________________________\n",
      "gmp1 (GlobalMaxPooling1D)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "pre_readout_drop_0.5 (Dropou (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "readout (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 189,545\n",
      "Trainable params: 189,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002609A1000D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Running iteration 2/12: {'layers': [128, 32, 8], 'conv': True, 'embed_size': 32}\n",
      "Model: \"functional_245\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "token_inputs (InputLayer)    [(None, 1000)]            0         \n",
      "_________________________________________________________________\n",
      "tokens_to_vectors (Embedding (None, 1000, 32)          704000    \n",
      "_________________________________________________________________\n",
      "c1conv_0_f8 (Conv1D)         (None, 333, 8)            776       \n",
      "_________________________________________________________________\n",
      "c1conv_1_f32 (Conv1D)        (None, 111, 32)           800       \n",
      "_________________________________________________________________\n",
      "c1conv_2_f128 (Conv1D)       (None, 37, 128)           12416     \n",
      "_________________________________________________________________\n",
      "gmp1 (GlobalMaxPooling1D)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "pre_readout_drop_0.5 (Dropou (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "readout (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 718,121\n",
      "Trainable params: 718,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000026084DA61F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Running iteration 3/12: {'layers': [128, 32, 8], 'conv': False, 'embed_size': 8}\n",
      "Model: \"functional_247\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "token_inputs (InputLayer)    [(None, 1000)]            0         \n",
      "_________________________________________________________________\n",
      "tokens_to_vectors (Embedding (None, 1000, 8)           176000    \n",
      "_________________________________________________________________\n",
      "matrix_to_vector (Flatten)   (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "linear_0_128 (Dense)         (None, 128)               1024128   \n",
      "_________________________________________________________________\n",
      "relu_0 (Activation)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "linear_1_32 (Dense)          (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "relu_1 (Activation)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "linear_2_8 (Dense)           (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "relu_2 (Activation)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "pre_readout_drop_0.5 (Dropou (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "readout (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 1,204,529\n",
      "Trainable params: 1,204,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002609ADE2EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Running iteration 4/12: {'layers': [128, 32, 8], 'conv': False, 'embed_size': 32}\n",
      "Model: \"functional_249\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "token_inputs (InputLayer)    [(None, 1000)]            0         \n",
      "_________________________________________________________________\n",
      "tokens_to_vectors (Embedding (None, 1000, 32)          704000    \n",
      "_________________________________________________________________\n",
      "matrix_to_vector (Flatten)   (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "linear_0_128 (Dense)         (None, 128)               4096128   \n",
      "_________________________________________________________________\n",
      "relu_0 (Activation)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "linear_1_32 (Dense)          (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "relu_1 (Activation)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "linear_2_8 (Dense)           (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "relu_2 (Activation)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "pre_readout_drop_0.5 (Dropou (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "readout (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 4,804,529\n",
      "Trainable params: 4,804,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000260E091D820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Running iteration 5/12: {'layers': [64], 'conv': True, 'embed_size': 8}\n",
      "Model: \"functional_251\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "token_inputs (InputLayer)    [(None, 1000)]            0         \n",
      "_________________________________________________________________\n",
      "tokens_to_vectors (Embedding (None, 1000, 8)           176000    \n",
      "_________________________________________________________________\n",
      "c1conv_0_f64 (Conv1D)        (None, 333, 64)           1600      \n",
      "_________________________________________________________________\n",
      "gmp1 (GlobalMaxPooling1D)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "pre_readout_drop_0.5 (Dropou (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "readout (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 177,665\n",
      "Trainable params: 177,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002608410D5E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Running iteration 6/12: {'layers': [64], 'conv': True, 'embed_size': 32}\n",
      "Model: \"functional_253\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "token_inputs (InputLayer)    [(None, 1000)]            0         \n",
      "_________________________________________________________________\n",
      "tokens_to_vectors (Embedding (None, 1000, 32)          704000    \n",
      "_________________________________________________________________\n",
      "c1conv_0_f64 (Conv1D)        (None, 333, 64)           6208      \n",
      "_________________________________________________________________\n",
      "gmp1 (GlobalMaxPooling1D)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "pre_readout_drop_0.5 (Dropou (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "readout (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 710,273\n",
      "Trainable params: 710,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002609A552DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Running iteration 7/12: {'layers': [64], 'conv': False, 'embed_size': 8}\n",
      "Model: \"functional_255\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "token_inputs (InputLayer)    [(None, 1000)]            0         \n",
      "_________________________________________________________________\n",
      "tokens_to_vectors (Embedding (None, 1000, 8)           176000    \n",
      "_________________________________________________________________\n",
      "matrix_to_vector (Flatten)   (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "linear_0_64 (Dense)          (None, 64)                512064    \n",
      "_________________________________________________________________\n",
      "relu_0 (Activation)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "pre_readout_drop_0.5 (Dropou (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "readout (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 688,129\n",
      "Trainable params: 688,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002609AE99790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Running iteration 8/12: {'layers': [64], 'conv': False, 'embed_size': 32}\n",
      "Model: \"functional_257\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "token_inputs (InputLayer)    [(None, 1000)]            0         \n",
      "_________________________________________________________________\n",
      "tokens_to_vectors (Embedding (None, 1000, 32)          704000    \n",
      "_________________________________________________________________\n",
      "matrix_to_vector (Flatten)   (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "linear_0_64 (Dense)          (None, 64)                2048064   \n",
      "_________________________________________________________________\n",
      "relu_0 (Activation)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "pre_readout_drop_0.5 (Dropou (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "readout (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 2,752,129\n",
      "Trainable params: 2,752,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002608C681160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Running iteration 9/12: {'layers': [64, 8], 'conv': True, 'embed_size': 8}\n",
      "Model: \"functional_259\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "token_inputs (InputLayer)    [(None, 1000)]            0         \n",
      "_________________________________________________________________\n",
      "tokens_to_vectors (Embedding (None, 1000, 8)           176000    \n",
      "_________________________________________________________________\n",
      "c1conv_0_f8 (Conv1D)         (None, 333, 8)            200       \n",
      "_________________________________________________________________\n",
      "c1conv_1_f64 (Conv1D)        (None, 111, 64)           1600      \n",
      "_________________________________________________________________\n",
      "gmp1 (GlobalMaxPooling1D)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "pre_readout_drop_0.5 (Dropou (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "readout (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 177,865\n",
      "Trainable params: 177,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002609E065160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Running iteration 10/12: {'layers': [64, 8], 'conv': True, 'embed_size': 32}\n",
      "Model: \"functional_261\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "token_inputs (InputLayer)    [(None, 1000)]            0         \n",
      "_________________________________________________________________\n",
      "tokens_to_vectors (Embedding (None, 1000, 32)          704000    \n",
      "_________________________________________________________________\n",
      "c1conv_0_f8 (Conv1D)         (None, 333, 8)            776       \n",
      "_________________________________________________________________\n",
      "c1conv_1_f64 (Conv1D)        (None, 111, 64)           1600      \n",
      "_________________________________________________________________\n",
      "gmp1 (GlobalMaxPooling1D)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "pre_readout_drop_0.5 (Dropou (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "readout (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 706,441\n",
      "Trainable params: 706,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002609FF38550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Running iteration 11/12: {'layers': [64, 8], 'conv': False, 'embed_size': 8}\n",
      "Model: \"functional_263\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "token_inputs (InputLayer)    [(None, 1000)]            0         \n",
      "_________________________________________________________________\n",
      "tokens_to_vectors (Embedding (None, 1000, 8)           176000    \n",
      "_________________________________________________________________\n",
      "matrix_to_vector (Flatten)   (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "linear_0_64 (Dense)          (None, 64)                512064    \n",
      "_________________________________________________________________\n",
      "relu_0 (Activation)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "linear_1_8 (Dense)           (None, 8)                 520       \n",
      "_________________________________________________________________\n",
      "relu_1 (Activation)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "pre_readout_drop_0.5 (Dropou (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "readout (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 688,593\n",
      "Trainable params: 688,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000026083221430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Running iteration 12/12: {'layers': [64, 8], 'conv': False, 'embed_size': 32}\n",
      "Model: \"functional_265\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "token_inputs (InputLayer)    [(None, 1000)]            0         \n",
      "_________________________________________________________________\n",
      "tokens_to_vectors (Embedding (None, 1000, 32)          704000    \n",
      "_________________________________________________________________\n",
      "matrix_to_vector (Flatten)   (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "linear_0_64 (Dense)          (None, 64)                2048064   \n",
      "_________________________________________________________________\n",
      "relu_0 (Activation)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "linear_1_8 (Dense)           (None, 8)                 520       \n",
      "_________________________________________________________________\n",
      "relu_1 (Activation)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "pre_readout_drop_0.5 (Dropou (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "readout (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,752,593\n",
      "Trainable params: 2,752,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002609B006B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MODEL</th>\n",
       "      <th>RESULT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[64, 8],C:False,E:32</td>\n",
       "      <td>0.212101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[128, 32, 8],C:False,E:8</td>\n",
       "      <td>0.217689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[128, 32, 8],C:False,E:32</td>\n",
       "      <td>0.230081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[64],C:False,E:8</td>\n",
       "      <td>0.239843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[64],C:True,E:8</td>\n",
       "      <td>0.252651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[64, 8],C:False,E:8</td>\n",
       "      <td>0.266339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[64, 8],C:True,E:32</td>\n",
       "      <td>0.269560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[64],C:False,E:32</td>\n",
       "      <td>0.291097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[64],C:True,E:32</td>\n",
       "      <td>0.296806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[64, 8],C:True,E:8</td>\n",
       "      <td>0.303942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[128, 32, 8],C:True,E:32</td>\n",
       "      <td>0.304634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 32, 8],C:True,E:8</td>\n",
       "      <td>0.322906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        MODEL    RESULT\n",
       "11       [64, 8],C:False,E:32  0.212101\n",
       "2    [128, 32, 8],C:False,E:8  0.217689\n",
       "3   [128, 32, 8],C:False,E:32  0.230081\n",
       "6            [64],C:False,E:8  0.239843\n",
       "4             [64],C:True,E:8  0.252651\n",
       "10        [64, 8],C:False,E:8  0.266339\n",
       "9         [64, 8],C:True,E:32  0.269560\n",
       "7           [64],C:False,E:32  0.291097\n",
       "5            [64],C:True,E:32  0.296806\n",
       "8          [64, 8],C:True,E:8  0.303942\n",
       "1    [128, 32, 8],C:True,E:32  0.304634\n",
       "0     [128, 32, 8],C:True,E:8  0.322906"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "results = {\n",
    "    'MODEL' : [],\n",
    "    'RESULT': [],\n",
    "}\n",
    "for grid_iter, grid_iter in enumerate(range(len(grid_combs))):\n",
    "    dct_curr_params = {k:v for k,v in zip(grid_params, grid_combs[grid_iter])}\n",
    "    print('Running iteration {}/{}: {}'.format(grid_iter+1,len(grid_combs), dct_curr_params))\n",
    "    model = model_factory(\n",
    "        inp=1_000,\n",
    "        vocab=len(w2i),\n",
    "        **dct_curr_params,\n",
    "    )\n",
    "    cb = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "    hist = model.fit(\n",
    "        x=x_trn, \n",
    "        y=y_trn, \n",
    "        validation_data=(x_dev, y_dev), \n",
    "        epochs=100, \n",
    "        batch_size=14, \n",
    "        callbacks=[cb],\n",
    "        verbose=0,\n",
    "    )\n",
    "    res = final_eval(\n",
    "        m=model, \n",
    "        x_test=x_test, \n",
    "        y_test=y_test\n",
    "    )\n",
    "    results['MODEL'].append(\"{},C:{},E:{}\".format(dct_curr_params['layers'],dct_curr_params['conv'],dct_curr_params['embed_size']))\n",
    "    results['RESULT'].append(res)\n",
    "pd.DataFrame(results).sort_values('RESULT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "95b969b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               MODEL    RESULT\n",
      "7  {'layers': [64, 16], 'conv': False, 'embed_siz...  0.206557\n",
      "6  {'layers': [64, 16], 'conv': False, 'embed_siz...  0.208043\n",
      "1  {'layers': [128], 'conv': True, 'embed_size': 32}  0.212973\n",
      "3  {'layers': [128], 'conv': False, 'embed_size':...  0.223192\n",
      "0  {'layers': [128], 'conv': True, 'embed_size': 10}  0.234100\n",
      "4  {'layers': [64, 16], 'conv': True, 'embed_size...  0.239393\n",
      "5  {'layers': [64, 16], 'conv': True, 'embed_size...  0.241737\n",
      "2  {'layers': [128], 'conv': False, 'embed_size':...  0.246123\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(results).sort_values('RESULT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e5a9cee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MODEL': [\"{'layers': [128], 'conv': True, 'embed_size': 10}\",\n",
       "  \"{'layers': [128], 'conv': True, 'embed_size': 32}\",\n",
       "  \"{'layers': [128], 'conv': False, 'embed_size': 10}\",\n",
       "  \"{'layers': [128], 'conv': False, 'embed_size': 32}\",\n",
       "  \"{'layers': [64, 16], 'conv': True, 'embed_size': 10}\",\n",
       "  \"{'layers': [64, 16], 'conv': True, 'embed_size': 32}\",\n",
       "  \"{'layers': [64, 16], 'conv': False, 'embed_size': 10}\",\n",
       "  \"{'layers': [64, 16], 'conv': False, 'embed_size': 32}\"],\n",
       " 'RESULT': [0.23409974093082986,\n",
       "  0.2129726093213153,\n",
       "  0.2461226547530063,\n",
       "  0.22319211295735722,\n",
       "  0.23939296224094905,\n",
       "  0.2417370345960695,\n",
       "  0.20804274040642304,\n",
       "  0.20655745336885573]}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7253b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
